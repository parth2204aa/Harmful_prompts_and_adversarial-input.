# Harmful_prompts_and_adversarial-input.
This project builds a test suite to evaluate how an AI model responds to harmful or adversarial prompts. It sends unsafe inputs (e.g., violent, toxic, or prompt injection attacks) to the model, analyzes the response using a toxicity detector (Detoxify), and flags failures when the model outputs harmful content.
